{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfcc416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\year4\\nlp\\project\\boardgame-scraping\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers torch scikit-learn numpy sentence-transformers ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df410098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import csv\n",
    "import json\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, utility, DataType\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c962f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Alibaba-NLP/gte-multilingual-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fc479b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bge_model = SentenceTransformer(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0196e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text):\n",
    "    # Use the SentenceTransformer model to generate embeddings\n",
    "    embeddings = bge_model.encode(text,convert_to_numpy=False)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea90063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MILVUS_HOST = 'localhost'\n",
    "MILVUS_PORT = '19530'\n",
    "\n",
    "connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f78a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecd7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize Milvus collection\n",
    "def initialize_milvus_collection(document_name):\n",
    "    # Check if collection exists\n",
    "    if not utility.has_collection(document_name):\n",
    "        # Create collection if it doesn't exist\n",
    "        # You may need to adjust the schema based on your specific requirements\n",
    "\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "            # FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1024),  # Adjust dim if needed\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=model_dim),  # Adjust dim if needed\n",
    "\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "        ]\n",
    "        schema = CollectionSchema(fields, \"Document embeddings for Information database\")\n",
    "        collection = Collection(document_name, schema)\n",
    "        \n",
    "        # Create an IVF_FLAT index for the embedding field\n",
    "        index_params = {\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"index_type\": \"IVF_FLAT\",\n",
    "            \"params\": {\"nlist\": model_dim}\n",
    "        }\n",
    "        collection.create_index(\"embedding\", index_params)\n",
    "    else:\n",
    "        collection = Collection(document_name)\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64b3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Milvus collection\n",
    "collection_name = 'note_book'\n",
    "collection = initialize_milvus_collection(collection_name)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aca4e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_milvus(text):\n",
    "    # Generate embeddings for the text\n",
    "    embedding = generate_embedding(text)\n",
    "\n",
    "    # Prepare the entity to be inserted\n",
    "    entity = {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "\n",
    "\n",
    "    # Insert the entity into Milvus\n",
    "    insert_result = collection.insert([entity])\n",
    "\n",
    "\n",
    "    # Ensure the changes are immediately searchable\n",
    "    collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a67830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_index_docs(docs_dir='../docs'):\n",
    "    \"\"\"\n",
    "    Read .txt files from the specified directory and index their contents.\n",
    "    \n",
    "    :param docs_dir: Directory containing the .txt files\n",
    "    :param index_url: URL of the indexing endpoint\n",
    "    \"\"\"\n",
    "    indexed_files = 0\n",
    "    non_indexed_files = 0\n",
    "    \n",
    "    selected_lines = ['Meta data', 'Content']\n",
    "    for filename in os.listdir(docs_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(docs_dir, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    \n",
    "                # Split content into chunks of max 1000 characters, including the title in each chunk\n",
    "                chunks = []\n",
    "                lines = content.split('\\n')\n",
    "                title = lines[0]  # Get the first line as the title\n",
    "                current_chunk = title + '\\n'  # Start each chunk with the title\n",
    "                for line in lines[1:]:  # Skip the first line (title) in this loop\n",
    "                    if any(line.startswith(prefix) for prefix in selected_lines):\n",
    "                        if len(current_chunk) + len(line) + 1 <= 1000:  # +1 for newline\n",
    "                            current_chunk += line + '\\n'\n",
    "                        else:\n",
    "                            if current_chunk:\n",
    "                                chunks.append(current_chunk.strip())\n",
    "                            current_chunk = title + '\\n' + line + '\\n'  # Start a new chunk with the title\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "\n",
    "                # Index each chunk separately\n",
    "                file_indexed = True\n",
    "                for chunk in chunks:\n",
    "                    insert_data_to_milvus(chunk)\n",
    "                \n",
    "                if file_indexed:\n",
    "                    indexed_files += 1\n",
    "                    print(f\"Finished indexing all chunks from {filename}\")\n",
    "                else:\n",
    "                    non_indexed_files += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "                non_indexed_files += 1\n",
    "\n",
    "    print(f\"Indexing complete. Indexed files: {indexed_files}, Non-indexed files: {non_indexed_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaf7f0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished indexing all chunks from 7 Wonders (TH) 7 สิ่งมหัศจรรย์.txt\n",
      "Finished indexing all chunks from 7 Wonders Architects (TH) 7 สิ่งมหัศจรรย์ ยอดสถาปนิก.txt\n",
      "Finished indexing all chunks from 8 BIT BOX (TH) 8 บิทบ๊อกซ์.txt\n",
      "Finished indexing all chunks from Art Society (TH) ศิลป์สโมสร.txt\n",
      "Finished indexing all chunks from Bandido (THEN) แบนดิโด.txt\n",
      "Finished indexing all chunks from Betakkuma  Fart & Furious ตดทะลุนรก.txt\n",
      "Finished indexing all chunks from Bloodbound (TH) สงครามแวมไพร์.txt\n",
      "Finished indexing all chunks from Bloody Inn (TH) โรงแรมสีเลือด.txt\n",
      "Finished indexing all chunks from Bloody Inn Carnies (TH) โรงแรมสีเลือด ชาวคณะหรรษา.txt\n",
      "Finished indexing all chunks from Book - Captive (TH).txt\n",
      "Finished indexing all chunks from Book - Knight (TH).txt\n",
      "Finished indexing all chunks from Book - Sherlock Holmes & Moriarty Associates (TH) เชอร์ล็อคโฮล์มส์ & มอริอาร์ตี้ พันธมิตรอันตราย.txt\n",
      "Finished indexing all chunks from Book - Sherlock Holmes four investigations (TH).txt\n",
      "Finished indexing all chunks from Book - Sherlock Holmes Shadows of Jack Ripper (TH) เชอร์ล็อค โฮล์มส์ เงามรณะของแจ็คเดอะริปเปอร์.txt\n",
      "Finished indexing all chunks from Bubbly (THEN) บับบลี.txt\n",
      "Finished indexing all chunks from Cash & Gun (TH) อย่าซ่ากับปืน.txt\n",
      "Finished indexing all chunks from Cat In The Box (THJP) เหมียวอินเดอะบ๊อกซ์.txt\n",
      "Finished indexing all chunks from Chinatown (TH) ไชน่าทาวน์.txt\n",
      "Finished indexing all chunks from Citadels Classic(TH) ศึกสร้างเมือง.txt\n",
      "Finished indexing all chunks from Coconut (TH) - ลิงยิงมะพร้าว.txt\n",
      "Finished indexing all chunks from Colt Express (TH) ขุมทรัพย์ม้าเหล็ก.txt\n",
      "Finished indexing all chunks from Coup (TH) เกมโค่นอำนาจ.txt\n",
      "Finished indexing all chunks from Dead of Winter (TH) เหมันต์มรณะ.txt\n",
      "Finished indexing all chunks from Dobble minions (TH) ด็อบเบิล มินเนี่ยน.txt\n",
      "Finished indexing all chunks from Down Force (TH) นักซิ่งสายฟ้า.txt\n",
      "Finished indexing all chunks from Dream On (TH) ทีมสร้างฝัน.txt\n",
      "Finished indexing all chunks from Exploding Kitten (TH) เหมียวระเบิด.txt\n",
      "Finished indexing all chunks from Exploding Kitten Party Pack (TH) เหมียวระเบิดปาร์ตี้แพ็ค.txt\n",
      "Finished indexing all chunks from Exploding Kittens Burglar Edition (TH) เหมียวระเบิด เมี้ยวขโมยเอดิชั่น.txt\n",
      "Finished indexing all chunks from Exploding Minions (TH) มินเนี่ยน ระเบิด.txt\n",
      "Finished indexing all chunks from Fallout Shelter (TH).txt\n",
      "Finished indexing all chunks from Farland (THEN) ฟาร์แลนด์.txt\n",
      "Finished indexing all chunks from Fearsome Floor (TH) เกมหนีผี! โถงมรณะ.txt\n",
      "Finished indexing all chunks from Feelink (TH) เกมปิ๊งใจ.txt\n",
      "Finished indexing all chunks from For Sale (TH) บ้านนี้ขาย!.txt\n",
      "Finished indexing all chunks from Forest (TH) ฟอเรสต์.txt\n",
      "Finished indexing all chunks from Friday (TH) ฟรายเดย์.txt\n",
      "Finished indexing all chunks from Game of Throne (TH) เกมล่าบัลลังก์.txt\n",
      "Finished indexing all chunks from Get Bit (TH) ฉลามงับ.txt\n",
      "Finished indexing all chunks from Gloomhaven  Jaw of The Lion (TH) กลูมเฮเวน  คมเขี้ยวราชสีห์.txt\n",
      "Finished indexing all chunks from Go Town (TH) โกทาวน์.txt\n",
      "Finished indexing all chunks from Hand of the King (TH) หัตถ์ราชา.txt\n",
      "Finished indexing all chunks from Hedgehog Roll (TH) เม่นม้วนผจญภัย.txt\n",
      "Finished indexing all chunks from I'm the Boss (TH) อย่าซ่ากับบอส.txt\n",
      "Finished indexing all chunks from Incan Gold (TH) ล่าสมบัติอินคา.txt\n",
      "Finished indexing all chunks from Inori อิโนริ.txt\n",
      "Finished indexing all chunks from Jaipur (TH) ชัยปุระ.txt\n",
      "Finished indexing all chunks from Kariba (TH) คาริบา.txt\n",
      "Finished indexing all chunks from Keyforge Age of Ascension - 2P Starter (TH) คีย์ฟอร์จยุคเรืองอำนาจ - ชุดเริ่มเล่น 2P.txt\n",
      "Finished indexing all chunks from Keyforge Age of Ascension - Decks (TH) คีย์ฟอร์จยุคเรืองอำนาจ - เด็ค.txt\n",
      "Finished indexing all chunks from Keyforge Mass Mutation - Decks (TH) คีย์ฟอร์จ กองทัพกลายพันธุ์ - เด็ค.txt\n",
      "Finished indexing all chunks from Keyforge Worlds Collide - Decks (TH) คีย์ฟอร์จ โลกาประจัญศึก - เด็ค.txt\n",
      "Finished indexing all chunks from Keyforge Worlds Collide - Deluxe (TH) คีย์ฟอร์จ โลกาประจัญศึก - Deluxe.txt\n",
      "Finished indexing all chunks from King of Tokyo (TH) ราชันแห่งโตเกียว.txt\n",
      "Finished indexing all chunks from King of Tokyo - Power Up (TH) ราชันแห่งโตเกียว พาวเวอร์อัป.txt\n",
      "Finished indexing all chunks from Kingdomino Origin (TH) คิงโดมิโน ออริจิน.txt\n",
      "Finished indexing all chunks from Little Town (TH) เมืองในฝัน.txt\n",
      "Finished indexing all chunks from Love Letter (TH) เกมจดหมายรัก.txt\n",
      "Finished indexing all chunks from Love Letter - 2025 Edition (TH) จดหมายรัก - ฉบับ 2025.txt\n",
      "Finished indexing all chunks from Mada (THEN) มาดา.txt\n",
      "Finished indexing all chunks from Mansion of Madness Expansion  Sanctum of Twilight (TH) คฤหาสน์วิปลาส วิหารลับแห่งสนธยา.txt\n",
      "Finished indexing all chunks from Micro Macro Crime City (TH) เมืองอาชญากรนครย่อส่วน.txt\n",
      "Finished indexing all chunks from Monster Eater Dungeon Meshi (TH) มอนสเตอร์อีทเตอร์ สูตรลับตำรับดันเจียน.txt\n",
      "Finished indexing all chunks from Next Station London (TH) สถานีต่อไป ลอนดอน.txt\n",
      "Finished indexing all chunks from Next Station Tokyo (TH) สถานีต่อไป โตเกียว.txt\n",
      "Finished indexing all chunks from Nyaice Code (TH) เหมียว Code.txt\n",
      "Finished indexing all chunks from One Night Daybreak (TH) ๑ คืนปริศนาเกมล่ามนุษย์หมาป่ารุ่งอรุณ.txt\n",
      "Finished indexing all chunks from One Night Ultimate Werewolf (TH) หนึ่งคืนปริศนาเกมล่ามนุษย์หมาป่า.txt\n",
      "Finished indexing all chunks from Pandemic (TH) เกมโรคระบาด.txt\n",
      "Finished indexing all chunks from Papageno (THEN) พาพากีโน.txt\n",
      "Finished indexing all chunks from Pengoloo (TH) เพนกวินหวงไข่.txt\n",
      "Finished indexing all chunks from Photosynthesis (TH) เกมสังเคราะห์แสง.txt\n",
      "Finished indexing all chunks from Point Salad (TH) สนุกสลัด.txt\n",
      "Finished indexing all chunks from Power Grid (TH) เกมโรงไฟฟ้า.txt\n",
      "Finished indexing all chunks from Power Hungry Pets (TH) ศึกป่วนก๊วนต้าวเหมียว.txt\n",
      "Finished indexing all chunks from Project L (TH) โปรเจค แอล.txt\n",
      "Finished indexing all chunks from Qubik (THEN) คิวบิค.txt\n",
      "Finished indexing all chunks from Ramen Extreme (TH) ราเมน สุดขีด!.txt\n",
      "Finished indexing all chunks from Rory's Story Cubes (TH) ลูกเต๋าเล่านิทาน.txt\n",
      "Finished indexing all chunks from Salem 1692 (TH) ซาเลม 1692.txt\n",
      "Finished indexing all chunks from Shadow House Masquerade (TH) ใครฆ่าเคานต์ คฤหาสน์เงา.txt\n",
      "Finished indexing all chunks from Sheriff of Nottingham (TH) ผู้ตรวจการแห่งเมืองนอตติงแฮม.txt\n",
      "Finished indexing all chunks from Skull (TH) สกัล.txt\n",
      "Finished indexing all chunks from Slide Quest (TH) อัศวินลมกรด.txt\n",
      "Finished indexing all chunks from Small World (TH) โลกใหญ่ใบเล็ก.txt\n",
      "Finished indexing all chunks from Splendor (TH) เกมค้าเพชร.txt\n",
      "Finished indexing all chunks from Splendor 2024 Edition (TH) เกมค้าเพชร ฉบับ 2024.txt\n",
      "Finished indexing all chunks from Sriracha (TH) ศรีราชา.txt\n",
      "Finished indexing all chunks from Superstore 3000 (TH) ซูเปอร์สโตร์ 3000.txt\n",
      "Finished indexing all chunks from Taco Cat Goat Chess Pizza (TH) ทาโก้ แมว แพะ ชีส พิซซ่า.txt\n",
      "Finished indexing all chunks from Tea For 2 (TH) ชา 2 ถ้วยในดินแดนมหัศจรรย์.txt\n",
      "Finished indexing all chunks from Telestrations (TH) วาด วาด ทาย ทาย.txt\n",
      "Finished indexing all chunks from Ticket to Ride (TH) เกมต่อรถไฟ.txt\n",
      "Finished indexing all chunks from Ticket to Ride New York (TH) เกมต่อรถตะลุยนิวยอร์ก.txt\n",
      "Finished indexing all chunks from Tikal (TH) ตีกัล.txt\n",
      "Finished indexing all chunks from Tiny Epic c(TH) มหากาพย์กลยุทธ์ฉบับจิ๋ว.txt\n",
      "Finished indexing all chunks from TM Colonies (TH) อาณานิคม.txt\n",
      "Finished indexing all chunks from TM Hellas & Elysium (TH) เฮลลาส & เอลิเซียม.txt\n",
      "Finished indexing all chunks from TM Prelude (TH) ปฐมบท.txt\n",
      "Finished indexing all chunks from TM Turmoil (TH) เกมการเมือง.txt\n",
      "Finished indexing all chunks from TM Venus Next (TH) ปฎิบัติการดาวศุกร์.txt\n",
      "Finished indexing all chunks from Toc Toc Woodman (TH) ต๊อก ต๊อก คนตัดไม้.txt\n",
      "Finished indexing all chunks from Travel Battle Fleet (TH) เกมแม่เหล็กกองเรือประจัญบาน.txt\n",
      "Finished indexing all chunks from Travel Checkers (TH) เกมแม่เหล็กหมากฮอส.txt\n",
      "Finished indexing all chunks from Travel Chess (TH) เกมแม่เหล็กหมากรุกสากล.txt\n",
      "Finished indexing all chunks from Travel Ludo (TH) เกมแม่เหล็กลูโด้.txt\n",
      "Finished indexing all chunks from Travel Reversi (TH) เกมแม่เหล็กโอเทลโล่.txt\n",
      "Finished indexing all chunks from Travel Secret Code (TH) เกมแม่เหล็กเกมรหัสลับ.txt\n",
      "Finished indexing all chunks from Travel Snakes (TH) เกมแม่เหล็กเกมบันไดงู.txt\n",
      "Finished indexing all chunks from Travel Tangram (TH) เกมแม่เหล็กแทนแกรม.txt\n",
      "Finished indexing all chunks from Travel Tic Tac Toe - Connect 4 (TH) เกมแม่เหล็กทิค แทค โท.txt\n",
      "Finished indexing all chunks from Tucano (TH) ทูคาโน.txt\n",
      "Finished indexing all chunks from Ultimate Werewolf Deluxe (TH) เกมล่าปริศนามนุษย์หมาป่า.txt\n",
      "Finished indexing all chunks from Ultimate Werewolf Expansion (TH) รวมภาคเสริมเกมล่าปริศนามนุษย์หมาป่า.txt\n",
      "Finished indexing all chunks from Ultimate Werewolf Extreme (TH) เกมล่าปริศนามนุษย์หมาป่า Extreme.txt\n",
      "Finished indexing all chunks from Unlock ! (TH) - อันล็อค.txt\n",
      "Finished indexing all chunks from Usagyuuun Coup (THEN) เกมโค่นอำนาจ อูซากูนนน.txt\n",
      "Finished indexing all chunks from Usagyuuun Crossing (THEN) ครอสซิ่ง อูซากูนนน.txt\n",
      "Finished indexing all chunks from Wallet (TH) เกมล่าตัวตนจารชนคนพันหน้า.txt\n",
      "Finished indexing all chunks from When I Dream (TH) นักท่องฝัน.txt\n",
      "Finished indexing all chunks from Who did it (TH) นี่อึใคร!.txt\n",
      "Finished indexing all chunks from Wingspan (TH) ปีกปักษา.txt\n",
      "Finished indexing all chunks from Wingspan Asia Expansion (TH) ปีกปักษา ภาคเสริม นกเอเชีย.txt\n",
      "Finished indexing all chunks from Wingspan Oceania Expansion (TH) ปีกปักษา ภาคเสริม นกโอเชียเนีย.txt\n",
      "Finished indexing all chunks from Winston (TH) วินสตัน.txt\n",
      "Finished indexing all chunks from Wyrmspan (TH) ปีกมังกร.txt\n",
      "Finished indexing all chunks from Zombie Kidz (TH) ขบวนการปราบซอมบี้.txt\n",
      "Finished indexing all chunks from [Pre-Order] Rumble Nation (TH) สงครามซามูไร.txt\n",
      "Finished indexing all chunks from [กล่องบุบ] 7 Wonders duel (TH) 7 สิ่งมหัศจรรย์ ดวล.txt\n",
      "Finished indexing all chunks from [กล่องบุบ] Dobble (TH) ด็อบเบิล.txt\n",
      "Finished indexing all chunks from [กล่องบุบ] Get Packing (TH).txt\n",
      "Finished indexing all chunks from [กล่องบุบ] Love Letter (TH) 2019 เกมจดหมายรัก.txt\n",
      "Finished indexing all chunks from [กล่องบุบ] Micro Macro Crime City Full House (TH) เมืองอาชญากรนครย่อส่วน ฟูลเฮาส์.txt\n",
      "Finished indexing all chunks from [กล่องบุบ] Power Grid Recharged (TH) เกมโรงไฟฟ้า Recharged.txt\n",
      "Finished indexing all chunks from [กล่องบุบ] Wingspan European Expansion (TH) ปีกปักษา ภาคเสริม นกยุโรป.txt\n",
      "Indexing complete. Indexed files: 135, Non-indexed files: 0\n"
     ]
    }
   ],
   "source": [
    "read_and_index_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb88de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completions(query):     \n",
    "    query_embedding = generate_embedding(query).numpy().flatten().tolist()\n",
    "    # Prepare search parameters\n",
    "    search_param = {\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"params\": {\"nprobe\": 10},\n",
    "    }\n",
    "    # Step 2: Retrieve top-10 documents from Milvus\n",
    "    search_results = collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_param,\n",
    "        limit=10,\n",
    "        output_fields=[\"id\", \"text\", \"embedding\"],\n",
    "        expr=None\n",
    "    )\n",
    "    \n",
    "    # Extract document texts and embeddings\n",
    "    retrieved_documents = []\n",
    "    document_embeddings = []\n",
    "    for hits in search_results:\n",
    "        for hit in hits:\n",
    "            logger.debug(f\"Retrieved document: {hit.entity.get('text')[:50]}...\")\n",
    "            retrieved_documents.append(hit.entity)\n",
    "            embedding = hit.entity.get('embedding')\n",
    "            logger.debug(f\"Retrieved embedding: {embedding[:5]}... (truncated)\")\n",
    "            if embedding is not None:\n",
    "                document_embeddings.append(embedding)\n",
    "                \n",
    "    return retrieved_documents, document_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
